{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Sec B - Aliza Ashfaq 22K-4566\n",
        "# Sec C - Misha Imam   22K-4179\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST data (you should download and place these files)\n",
        "def load_mnist_images(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        f.read(16)\n",
        "        data = np.frombuffer(f.read(), np.uint8)\n",
        "    return data.reshape(-1, 28, 28).astype(np.float32) / 255.0\n",
        "\n",
        "def load_mnist_labels(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        f.read(8)\n",
        "        labels = np.frombuffer(f.read(), np.uint8)\n",
        "    return labels\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(predictions, labels):\n",
        "    m = labels.shape[0]\n",
        "    p = predictions[range(m), labels]\n",
        "    return -np.mean(np.log(p + 1e-9))\n",
        "\n",
        "def cross_entropy_derivative(predictions, labels):\n",
        "    m = labels.shape[0]\n",
        "    grad = predictions.copy()\n",
        "    grad[range(m), labels] -= 1\n",
        "    return grad / m\n",
        "\n",
        "def convolve2d(image, kernel, stride=1):\n",
        "    output_size = (image.shape[0] - kernel.shape[0]) // stride + 1\n",
        "    output = np.zeros((output_size, output_size))\n",
        "    for i in range(0, output_size):\n",
        "        for j in range(0, output_size):\n",
        "            output[i, j] = np.sum(image[i*stride:i*stride+3, j*stride:j*stride+3] * kernel)\n",
        "    return output\n",
        "\n",
        "def convolve_batch(batch, kernel):\n",
        "    return np.array([convolve2d(img, kernel) for img in batch])\n",
        "\n",
        "def max_pool(image, size=2, stride=2):\n",
        "    out_h = image.shape[0] // size\n",
        "    out_w = image.shape[1] // size\n",
        "    output = np.zeros((out_h, out_w))\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            output[i, j] = np.max(image[i*stride:i*stride+size, j*stride:j*stride+size])\n",
        "    return output\n",
        "\n",
        "def max_pool_batch(batch):\n",
        "    return np.array([max_pool(img) for img in batch])\n",
        "\n",
        "def flatten(batch):\n",
        "    return batch.reshape(batch.shape[0], -1)\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "# Load data\n",
        "x_train = load_mnist_images(\"train-images.idx3-ubyte\")\n",
        "y_train = load_mnist_labels(\"train-labels.idx1-ubyte\")\n",
        "x_test = load_mnist_images(\"t10k-images.idx3-ubyte\")\n",
        "y_test = load_mnist_labels(\"t10k-labels.idx1-ubyte\")\n",
        "\n",
        "# Initialize weights\n",
        "filter1 = np.random.randn(3, 3) * 0.1\n",
        "filter2 = np.random.randn(3, 3) * 0.1\n",
        "\n",
        "input_size = ((28 - 2*2) // 2)**2  # after 2 convolutions and pooling\n",
        "input_size = (input_size * 1)  # 1 filter output channel\n",
        "weights_fc = np.random.randn(input_size, 10) * 0.1\n",
        "biases_fc = np.zeros(10)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, x_train.shape[0], batch_size):\n",
        "        batch_imgs = x_train[i:i+batch_size]\n",
        "        batch_labels = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        conv1 = convolve_batch(batch_imgs, filter1)\n",
        "        relu1 = relu(conv1)\n",
        "        conv2 = convolve_batch(relu1, filter2)\n",
        "        relu2 = relu(conv2)\n",
        "        pool = max_pool_batch(relu2)\n",
        "        flat = flatten(pool)\n",
        "        logits = flat @ weights_fc + biases_fc\n",
        "        probs = softmax(logits)\n",
        "        loss = cross_entropy_loss(probs, batch_labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        dL_dlogits = cross_entropy_derivative(probs, batch_labels)\n",
        "        dL_dw_fc = flat.T @ dL_dlogits\n",
        "        dL_db_fc = np.sum(dL_dlogits, axis=0)\n",
        "        dL_dflat = dL_dlogits @ weights_fc.T\n",
        "\n",
        "        # Update fully connected weights\n",
        "        weights_fc -= learning_rate * dL_dw_fc\n",
        "        biases_fc -= learning_rate * dL_db_fc\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
        "\n",
        "# Evaluate accuracy on test set\n",
        "test_conv1 = convolve_batch(x_test, filter1)\n",
        "test_relu1 = relu(test_conv1)\n",
        "test_conv2 = convolve_batch(test_relu1, filter2)\n",
        "test_relu2 = relu(test_conv2)\n",
        "test_pool = max_pool_batch(test_relu2)\n",
        "test_flat = flatten(test_pool)\n",
        "test_logits = test_flat @ weights_fc + biases_fc\n",
        "test_probs = softmax(test_logits)\n",
        "test_preds = np.argmax(test_probs, axis=1)\n",
        "accuracy = np.mean(test_preds == y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "RcAqC741A3Op"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}